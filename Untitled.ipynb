{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tracked-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shubham\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shubham\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shubham\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', '19', 'sample', 'data', 'analysis', 'is', 'very', 'difficult', '(', 'also', 'known', 'as', 'hard', ')', 'and', 'time', 'consuming', '.']\n",
      "['H', 'e', 'r', 'e', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'v', 'i', 'd', ' ', 'd', 'a', 't', 'a', ' ', 'f', 'o', 'r', ' ', '2', '0', '1', '9', ' ', '2', '0', '2', '0']\n",
      "Heres is the covid data for 2019 2020\n",
      "['Heres', 'is', 'the', 'covid', 'data', 'for', '2019', '2020']\n",
      "{'heres': 5, 'is': 6, 'the': 7, 'covid': 2, 'data': 3, 'for': 4, '2019': 0, '2020': 1}\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "0       [Positive]\n",
      "1       [Negative]\n",
      "2       [Negative]\n",
      "3        [Neutral]\n",
      "4        [Neutral]\n",
      "           ...    \n",
      "9995    [Negative]\n",
      "9996    [Positive]\n",
      "9997    [Negative]\n",
      "9998    [Negative]\n",
      "9999    [Positive]\n",
      "Name: Sentiment, Length: 10000, dtype: object\n",
      "  (0, 422)\t1\n",
      "  (0, 1407)\t1\n",
      "  (0, 1815)\t1\n",
      "  (0, 5631)\t1\n",
      "  (0, 7521)\t1\n",
      "  (0, 13155)\t2\n",
      "  (0, 13180)\t1\n",
      "  (0, 13329)\t1\n",
      "  (0, 13679)\t2\n",
      "  (0, 13861)\t1\n",
      "  (0, 14008)\t1\n",
      "  (0, 14263)\t1\n",
      "  (0, 14461)\t1\n",
      "  (0, 15261)\t1\n",
      "  (0, 23432)\t1\n",
      "  (0, 26856)\t1\n",
      "  (0, 28338)\t1\n",
      "  (0, 29287)\t1\n",
      "  (0, 29458)\t1\n",
      "  (0, 29460)\t1\n",
      "  (0, 29641)\t1\n",
      "  (1, 10314)\t1\n",
      "  (1, 10365)\t1\n",
      "  (1, 10891)\t1\n",
      "  (1, 11841)\t1\n",
      "  :\t:\n",
      "  (9998, 28150)\t1\n",
      "  (9998, 28815)\t1\n",
      "  (9998, 28963)\t1\n",
      "  (9998, 29512)\t1\n",
      "  (9998, 29830)\t1\n",
      "  (9999, 940)\t1\n",
      "  (9999, 3317)\t1\n",
      "  (9999, 4240)\t1\n",
      "  (9999, 5276)\t1\n",
      "  (9999, 10757)\t1\n",
      "  (9999, 11718)\t1\n",
      "  (9999, 12859)\t1\n",
      "  (9999, 13736)\t1\n",
      "  (9999, 14960)\t1\n",
      "  (9999, 15038)\t1\n",
      "  (9999, 15041)\t1\n",
      "  (9999, 15468)\t2\n",
      "  (9999, 17049)\t1\n",
      "  (9999, 23345)\t1\n",
      "  (9999, 23371)\t1\n",
      "  (9999, 23384)\t1\n",
      "  (9999, 27013)\t1\n",
      "  (9999, 27960)\t1\n",
      "  (9999, 28069)\t1\n",
      "  (9999, 29830)\t1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.26 GiB for an array with shape (10000, 30321) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6850b31fb950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_bow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shubham gupta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shubham gupta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.26 GiB for an array with shape (10000, 30321) and data type int64"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('tweets_tagged_new_format.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df['Sentiment'].value_counts() #positive, negative , neutral\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data = \"Covid 19 sample data analysis is very difficult (also known as hard) and time consuming.\"\n",
    "print(word_tokenize(data))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "import string \n",
    "data = \"Here's is the covid data for 2019, 2020!!!\"\n",
    "data_1=[char for char in data if char not in string.punctuation]\n",
    "print(data_1)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "data_1=''.join(data_1)\n",
    "print (data_1)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "data_1 = data_1.split()\n",
    "word_tokenize(data)\n",
    "print(data_1)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data_1 = [\"Heres is the covid data for 2019 2020.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(data_1)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "\"\"\"encode document\"\"\"\n",
    "vector = vectorizer.transform(data_1)\n",
    "print(vector)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def text_cleaning(a):\n",
    "    remove_punctuation = [char for char in a if char not in string.punctuation]\n",
    "    #print(remove_punctuation)\n",
    "    remove_punctuation=''.join(remove_punctuation)\n",
    "    #print(remove_punctuation)\n",
    "    return [word for word in remove_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "print(df.iloc[:,2].apply(text_cleaning))\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=text_cleaning).fit(df['Tweet'])\n",
    "\n",
    "#print(len(bow_transformer.vocabulary_))\n",
    "bow_transformer.vocabulary_\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "title_bow = bow_transformer.transform(df['Tweet'])\n",
    "print(title_bow)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "x = title_bow.toarray()\n",
    "print(x)\n",
    "\n",
    "x.shape\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(title_bow)\n",
    "print(tfidf_transformer)\n",
    "\n",
    "title_tfidf = tfidf_transformer.transform(title_bow)\n",
    "print(title_tfidf)\n",
    "print(title_tfidf.shape)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(title_tfidf, df['Sentiment'])\n",
    "#model_1 = MultinomialNB().get_params(['Tweet'])\n",
    "#print(model_1)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "all_predictions = model.predict(title_tfidf)\n",
    "print(all_predictions)\n",
    "#all_predictions2 = model.predict_log_proba(title_tfidf)\n",
    "#print(all_predictions2)\n",
    "\n",
    "#all_predictions3 = model.predict_proba(title_tfidf)\n",
    "#print(all_predictions3)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(df['Sentiment'], all_predictions))\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(all_predictions,df['Sentiment']))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(all_predictions,df['Sentiment']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-miller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
